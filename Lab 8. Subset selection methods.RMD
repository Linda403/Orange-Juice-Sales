---
title: 'DSO 530: Orange Juice Case'
author: "DSO 530"
date: "11/9/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Orange Juice Sales

We will analyse the weekly sales data of refrigerated 64-ounce orange juice
containers from 83 stores in the Chicago area. There are many stores throughout
the city, many time periods, and three brands (Dominicks, MinuteMaid, and Tropicana).
The data are arranged in rows with each row giving the recorded sales (in
logarithms; `logmove`), as well as brand, price, presence/absence of feature advertisement,
and the demographic characteristics of the stores. In total, there are 28,947
rows in this data set. The data is taken from P. Rossi’s bayesm package for R, and
it has been used earlier in Montgomery (1987).

### Data

---
STORE \  \ store number

BRAND \  \ brand indicator

WEEK \  \ week number

LOGMOVE \  \ log of the number of 64oz units sold

PRICE \  \ price of 64oz unit

FEATURE \  \ feature advertisement

AGE60 \ \  proportion of the population that is aged 60 or older

EDUC \  \ proportion of the population that has a college degree

ETHNIC \  \ proportion of the population that is black or Hispanic

INCOME \  \  log median income

HHLARGE \  \ proportion of households with 5 or more persons

WORKWOM \  \  proportion of women with full-time jobs

HVAL150 \  \ proportion of households worth more than $150,000

SSTRDIST \  \ distance to the nearest warehouse store

SSTRVOL \  \ ratio of sales of this store to the nearest warehouse store

CPDIST5 \  \ average distance in miles to the nearest 5 supermarkets

CPWVOL5 \  \ ratio of sales of this store to the average of the nearest
five stores

### Task

Build a model that predicts the sales (log of the number of units sold) using ridge and lasso regression models. Select the best one, i.e. the one with the lowest testing MSE.

### Steps

@. Upload the data from `oj.csv` file, call it `oj`. Convert the variable `store` to a categorical factor: `oj$store <- factor(oj$store)`. 

```{r}
oj <- read.csv("oj.csv")
attach(oj)
oj$store <- factor(oj$store)
```

@. Specify the model that contains as explanatory variables the logarithm
of price and its interaction with linear and quadratic components for feature, brand,
and the demographic characteristics of a store’s neighborhood. Price elasticities are most likely affected by demographic characteristics such as the average income of a store’s immediate neighborhood.

The `model.matrix` statement in `R` allows us to specify the model without having
to write out all its terms in detail. The model `y ∼ z ∗ (x1 + x2 + x3)∧2`, for
example, includes the intercept and the following 13 terms: `z, x1, x2, x3, x1 ∗
x2, x1 ∗ x3, x2 ∗ x3, z ∗ x1, z ∗ x2, z ∗ x3, z ∗ x1 ∗ x2, z ∗ x1 ∗ x3, z ∗ x2 ∗ x3`.

Our model, with the three brands represented by two indicator variables, contains
210 factors (including the intercept). This is a very large number, suggesting
a shrinkage approach such as Ridge and LASSO for the estimation of its parameters.

Make a model matrix as shown below. Remove the first column.

```{r}
x <- model.matrix(logmove ~ log(price)*(feat + brand 
                                        + AGE60 + EDUC + ETHNIC + INCOME + HHLARGE + WORKWOM 
                                        + HVAL150 + SSTRDIST + SSTRVOL + CPDIST5 + CPWVOL5)^2,
                  data=oj)
```


@. Normalize the variables as they are of very different magnitudes, and we transform them such that each variable has mean 0 and standard deviation 1. Use function `scale()`, specify both `center=` and `scale=` options as `TRUE`

```{r}
#?scale
scale(x,center = TRUE, scale = TRUE)
```


@. Split the data into training and testing sets. Use `set.seed(1234)` to split the data into training and testing sets.
Let the training set be k=1,000, the rest should go into testing set.

```{r}
set.seed(1234)
k=1000
train=sample(1:nrow(oj), k)
test=-train
y=oj$logmove
y.test=oj$logmove[test]
```

@. Use the training set to build a ridge regression model with an optimal lambda (use 10-fold cross-validation method to find it). Use the following values of lambda to find an optimal: `grid=10^seq(1,-3,length=10)`. Report the testing MSE of this model.

```{r}
ridge.mod=glmnet(x[train,], y[train], 
                 alpha = 0, lambda = grid, 
                 thresh = 1e-12)

grid=10^seq(1,-3,length=10)
ridge.mod=glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred=predict(ridge.mod, s=4, newx = x[test,])

```

@. Repeat the same steps as in previous part to find testing MSS for the lasso regression model.
```{r}
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)

set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod, s=bestlam, newx=x[test,])

```


@. Which model would you choose to predict the orange juice sales?
```{r}
mean((ridge.pred-y.test)^2)
mean((lasso.pred-y.test)^2)
mean((mean(y[train])-y.test)^2)
```

I would use the lasso regression to predict the orange juice sales because it has lower error rate.

